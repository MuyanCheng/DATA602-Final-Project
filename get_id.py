# -*- coding: utf-8 -*-
"""Get ID.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hlw1pJhQnR7e8YQdSv33iWiYqySTd-nF
"""

#-----General------#
import numpy as np
import pandas as pd
import os
import sys
import math
import random
import string
#-----Plotting-----#
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
import plotly.offline as py
py.init_notebook_mode(connected=True)
import seaborn as sns
# from pandas_profiling import ProfileReport

#-----Utility-----#
import itertools
import warnings
warnings.filterwarnings("ignore")
import re
import gc
from bs4 import BeautifulSoup as soup
from urllib.request import Request, urlopen
from datetime import date, datetime
LOOK_AT = 5 # Controls how many bars the user can see in the bar graph
AT_LEAST = 50 # Controls what rank a country must be in terms of total cases to be shown on the bar graph
def get_id(url, id_list):
  req = Request(url)
  webpage_top_50 = urlopen(req)
  page_soup = soup(webpage_top_50, "lxml")
  mydivs = page_soup.findAll("div",{"class":"contact-buttons"})
  #print(mydivs)
  i = 1
  for i in mydivs:
    tp = i.get('id')[:-10]
    if len(tp)!=36:
      tp = tp[10:]
    id_list.append(tp)

zip_code_NC = []
zip_code_SC = []
zip_code_GA = []
zip_code_FL = []

url = 'https://www.ciclt.net/sn/clt/capitolimpact/gw_ziplist.aspx?zip=300'
req = Request(url)
webpage_top_50 = urlopen(req)
page_soup = soup(webpage_top_50, "lxml")
mydivs = page_soup.findAll("table")
content = mydivs[0].findAll("tr")
content.pop(0)
content.pop(0)
content.pop(0)
content.pop(0)
for i in content:
  zip_code_GA.append(i.text[:5])

combine = []

#cleaned_VT_zip = []
#cleaned_NH_zip = []
#cleaned_MN_zip = []
#cleaned_WV_zip = []
for i in zip_code_FL:
  if i not in combine:
    combine.append(i)

cleaned_OH_zip.pop(49)
#print(cleaned_OH_zip[49])
for i in cleaned_OH_zip:
  print(i)
#cleaned_VT_zip.pop(46)
#print(cleaned_NY_zip[44])

file1 = open('south_zip_list.txt', 'r')
Lines = file1.readlines()
for i in Lines:
  combine.append(i[:-1])
#file2 = open('second_part(start23).txt', 'r')
#Lines = file2.readlines()
#for i in Lines:
  #a = i[:-1]
  #raw.append(i)

zip_code = [20001,20002,20003,20004,20005,20006,20007,20008,20009,20010,20011,20012,20015,20016,20017,20018,20019, 20020, 20024, 20032, 20036, 20037, 20045, 20052, 20053, 20057, 20064, 20202, 20204, 20228, 20230, 20240, 20245, 20260, 20307, 20317, 20319, 20373, 20390, 20405, 20418, 20427, 20506, 20510, 20520, 20535, 20540, 20551, 20553, 20560, 20565, 20566, 20593]

data1 = []
data2 = []

c = 0
for i in ids:
  if i not in cleaned:
    data1.append(i)
  print(c)
  c = c+1

a = 0
for i in ids:
  if i not in data2:
    data2.append(i)
  print((a/2035762)*100, "%")
  a = a + 1

first = 'https://www.cars.com/shopping/results/?dealer_id=&keyword=&list_price_max=&list_price_min=&makes[]=&maximum_distance=50&mileage_max=&monthly_payment=&page='
second = '&page_size=100&sort=best_match_desc&stock_type=used&year_max=&year_min=&zip='
for j in range(150,200):
  for i in range(1,101):
    url = first+str(i)+second+str(combine[j])
    get_id(url,data1)
    print(j, i)

for i in new_clean:
  cleaned.append(i)

with open('south_id(150-200).txt', 'w') as f:
    for i in data1:
        f.write(i+"\n")